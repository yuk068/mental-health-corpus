{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c752c73a-db80-4288-b808-79428c06cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Cleaned Corpus (23240, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cleaned_corpus = pd.read_csv('data/cleaned_mhc.csv')\n",
    "\n",
    "print(\"Shape of Cleaned Corpus\", cleaned_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea9b498-cb5c-4825-9050-00277e70f66f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 sample:\n",
      "recently ive eating stomach problem im getting really bad stomach pain im hungry appetite throw little food eat think eating disorder something im sure\n",
      "\n",
      "TF-IDF values:\n",
      "    Token   TF-IDF\n",
      "  stomach 0.514720\n",
      "   eating 0.434127\n",
      "   hungry 0.286663\n",
      "    throw 0.235316\n",
      " disorder 0.226459\n",
      "       im 0.225463\n",
      "     food 0.214009\n",
      "      eat 0.211527\n",
      " recently 0.184122\n",
      "  problem 0.158726\n",
      "     pain 0.154161\n",
      "     sure 0.152481\n",
      "   little 0.150323\n",
      "  getting 0.141145\n",
      "      bad 0.135555\n",
      "something 0.126122\n",
      "      ive 0.106503\n",
      "    think 0.105709\n",
      "   really 0.104681\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 19\n",
      "Zero values: 3481\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 0 sample:\n",
      "come rteenagers let people farm karma make post making fun karma get deleted karma ridiculous let meme ugh\n",
      "\n",
      "TF-IDF values:\n",
      "     Token   TF-IDF\n",
      "     karma 0.730628\n",
      "       let 0.266933\n",
      "       ugh 0.264021\n",
      "rteenagers 0.249057\n",
      "ridiculous 0.238737\n",
      "   deleted 0.238392\n",
      "      meme 0.215462\n",
      "       fun 0.159209\n",
      "    making 0.144351\n",
      "      post 0.135817\n",
      "      come 0.124612\n",
      "      make 0.093020\n",
      "    people 0.088661\n",
      "       get 0.079184\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 14\n",
      "Zero values: 3486\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 0 sample:\n",
      "im true nerf war veteran im real even call pro use poop battle cum substitute bullet\n",
      "\n",
      "TF-IDF values:\n",
      "  Token   TF-IDF\n",
      "veteran 0.431727\n",
      "    pro 0.401784\n",
      " bullet 0.380166\n",
      " battle 0.335614\n",
      "    war 0.316871\n",
      "   true 0.265478\n",
      "    use 0.247315\n",
      "   call 0.233658\n",
      "   real 0.221305\n",
      "     im 0.202540\n",
      "   even 0.135550\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 11\n",
      "Zero values: 3489\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 0 sample:\n",
      "day guess song added word today ill add another word tommorow maybe winner today let\n",
      "\n",
      "TF-IDF values:\n",
      "  Token   TF-IDF\n",
      "   word 0.469420\n",
      " winner 0.398136\n",
      "  today 0.385011\n",
      "  added 0.333273\n",
      "    add 0.287685\n",
      "   song 0.274502\n",
      "  guess 0.211207\n",
      "another 0.194518\n",
      "    let 0.191901\n",
      "  maybe 0.191363\n",
      "    ill 0.172067\n",
      "    day 0.134964\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 12\n",
      "Zero values: 3488\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 0 sample:\n",
      "cool fact house cat god bone crushed god help hear demon scream god look eye\n",
      "\n",
      "TF-IDF values:\n",
      "  Token   TF-IDF\n",
      "    god 0.595765\n",
      "crushed 0.298841\n",
      "   bone 0.298317\n",
      "  demon 0.295297\n",
      " scream 0.272165\n",
      "    cat 0.245386\n",
      "   cool 0.228672\n",
      "    eye 0.205675\n",
      "   hear 0.204479\n",
      "   fact 0.185619\n",
      "  house 0.185360\n",
      "   look 0.157061\n",
      "   help 0.124058\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 13\n",
      "Zero values: 3487\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 1 sample:\n",
      "really want die also dont want leave anyone behind know wouldnt make really suicidal worse\n",
      "\n",
      "TF-IDF values:\n",
      "   Token   TF-IDF\n",
      " wouldnt 0.398240\n",
      "  really 0.364489\n",
      "  behind 0.352312\n",
      "    want 0.304147\n",
      "   leave 0.290301\n",
      "   worse 0.273270\n",
      "suicidal 0.271695\n",
      "     die 0.231617\n",
      "    also 0.230504\n",
      "  anyone 0.222146\n",
      "    dont 0.211612\n",
      "    make 0.186051\n",
      "    know 0.153725\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 13\n",
      "Zero values: 3487\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 1 sample:\n",
      "want anyone try help want die want die people give shit would fuckin life one decides die\n",
      "\n",
      "TF-IDF values:\n",
      "  Token   TF-IDF\n",
      "    die 0.552861\n",
      " fuckin 0.405786\n",
      "decides 0.390141\n",
      "   want 0.362994\n",
      "   shit 0.194371\n",
      "   give 0.194101\n",
      "    try 0.193930\n",
      " anyone 0.176752\n",
      "   help 0.163358\n",
      "  would 0.144864\n",
      " people 0.141096\n",
      "    one 0.131218\n",
      "   life 0.128862\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 13\n",
      "Zero values: 3487\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 1 sample:\n",
      "even fucking know anymoremy grade failing relationship family abusive want live anymore keep trying best never work\n",
      "\n",
      "TF-IDF values:\n",
      "       Token   TF-IDF\n",
      "     failing 0.383907\n",
      "     abusive 0.378687\n",
      "       grade 0.332009\n",
      "relationship 0.283297\n",
      "      trying 0.243750\n",
      "        best 0.237846\n",
      "     fucking 0.233226\n",
      "        keep 0.229988\n",
      "        work 0.219751\n",
      "        live 0.218817\n",
      "      family 0.216800\n",
      "     anymore 0.210811\n",
      "       never 0.190571\n",
      "        even 0.168312\n",
      "        know 0.147742\n",
      "        want 0.146155\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 16\n",
      "Zero values: 3484\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 1 sample:\n",
      "cant muster courage kill day want jump skin life torturing day every day cant possibly make another week want die soo badly im really scared\n",
      "\n",
      "TF-IDF values:\n",
      "   Token   TF-IDF\n",
      "     day 0.380921\n",
      "     soo 0.368451\n",
      "    skin 0.293777\n",
      "possibly 0.285964\n",
      " courage 0.273757\n",
      "   badly 0.269823\n",
      "    jump 0.268523\n",
      "    cant 0.246963\n",
      "    want 0.205696\n",
      "  scared 0.199642\n",
      " another 0.183001\n",
      "    week 0.175897\n",
      "    kill 0.158003\n",
      "     die 0.156644\n",
      "   every 0.152545\n",
      "    make 0.125828\n",
      "  really 0.123253\n",
      "    life 0.109533\n",
      "      im 0.088487\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 19\n",
      "Zero values: 3481\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n",
      "Class 1 sample:\n",
      "gun redflag killed boyfriend gun would get trouble law im considering option want cause legal problem im gone\n",
      "\n",
      "TF-IDF values:\n",
      "      Token   TF-IDF\n",
      "        gun 0.503507\n",
      "      legal 0.332282\n",
      "        law 0.313152\n",
      "considering 0.269671\n",
      "    trouble 0.266998\n",
      "     killed 0.243685\n",
      "     option 0.241319\n",
      "  boyfriend 0.234734\n",
      "      cause 0.221689\n",
      "       gone 0.204241\n",
      "    problem 0.187572\n",
      "         im 0.177625\n",
      "    redflag 0.161851\n",
      "      would 0.123587\n",
      "        get 0.107506\n",
      "       want 0.103226\n",
      "\n",
      "Vector statistics:\n",
      "Non-zero values: 16\n",
      "Zero values: 3484\n",
      "Total vector length: 3500\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3500)\n",
    "\n",
    "# Fit TF-IDF vectorizer\n",
    "X = vectorizer.fit_transform(cleaned_corpus['text'])\n",
    "\n",
    "# Get samples from each class\n",
    "class_0 = cleaned_corpus[cleaned_corpus['label'] == 0]\n",
    "class_1 = cleaned_corpus[cleaned_corpus['label'] == 1]\n",
    "\n",
    "# Filter samples by token length\n",
    "mask_0 = class_0['text'].str.split().str.len().between(15, 25)\n",
    "mask_1 = class_1['text'].str.split().str.len().between(15, 25)\n",
    "\n",
    "samples_0 = class_0[mask_0].sample(n=5, random_state=42)\n",
    "samples_1 = class_1[mask_1].sample(n=5, random_state=42)\n",
    "\n",
    "samples = pd.concat([samples_0, samples_1])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "vector_length = len(feature_names)\n",
    "\n",
    "for idx, row in samples.iterrows():\n",
    "    print(f\"Class {row['label']} sample:\")\n",
    "    print(f\"{row['text']}\\n\")\n",
    "    \n",
    "    print(\"TF-IDF values:\")\n",
    "    sample_vector = vectorizer.transform([row['text']])\n",
    "    vector_array = sample_vector.toarray()[0]\n",
    "    nonzero_mask = vector_array != 0\n",
    "    nonzero_values = pd.DataFrame({\n",
    "        'Token': feature_names[nonzero_mask],\n",
    "        'TF-IDF': vector_array[nonzero_mask]\n",
    "    })\n",
    "    nonzero_values = nonzero_values.sort_values('TF-IDF', ascending=False)\n",
    "    print(nonzero_values.to_string(index=False))\n",
    "    \n",
    "    n_nonzero = np.count_nonzero(vector_array)\n",
    "    n_zero = vector_length - n_nonzero\n",
    "    print(f\"\\nVector statistics:\")\n",
    "    print(f\"Non-zero values: {n_nonzero}\")\n",
    "    print(f\"Zero values: {n_zero}\")\n",
    "    print(f\"Total vector length: {vector_length}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7beaf4-4bd4-4669-966f-745e62a68f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         term        score\n",
      "1564       im  1379.222785\n",
      "1812     like   901.153227\n",
      "3355     want   872.305628\n",
      "1167     feel   769.756417\n",
      "1740     know   724.344765\n",
      "1806     life   677.561486\n",
      "1318      get   634.966980\n",
      "2135      one   574.847310\n",
      "450      cant   566.188210\n",
      "2219   people   565.507400\n",
      "3152     time   558.566592\n",
      "1676      ive   555.463606\n",
      "1270   friend   542.760670\n",
      "908      dont   539.305194\n",
      "2470   really   517.343020\n",
      "3456    would   516.749624\n",
      "1056     even   503.161687\n",
      "3478     year   490.136575\n",
      "767       day   489.172177\n",
      "3119    think   479.044374\n",
      "3118    thing   455.077405\n",
      "1332       go   451.763167\n",
      "1336    going   437.458096\n",
      "1891     make   432.948721\n",
      "1462     help   430.563804\n",
      "2025     much   394.223357\n",
      "2071    never   393.593957\n",
      "1341     good   386.576097\n",
      "2059     need   366.305285\n",
      "1281  fucking   363.305309\n",
      "            term     score\n",
      "1883     maddock  0.708841\n",
      "1400         haa  0.909182\n",
      "542        clack  0.999677\n",
      "113        ameno  1.000000\n",
      "390        brice  1.016318\n",
      "1803        lick  4.350532\n",
      "3375     wealthy  4.418583\n",
      "797     delivers  4.513737\n",
      "92          alex  4.735390\n",
      "1486   hitchcock  4.781842\n",
      "128         anne  4.801267\n",
      "1685       jason  4.901999\n",
      "1483  historical  4.958354\n",
      "2202  passionate  4.963529\n",
      "202    associate  4.965971\n",
      "218    attending  4.972752\n",
      "1576  importance  4.983625\n",
      "2568    revealed  5.000154\n",
      "1353     grabbed  5.119948\n",
      "555        cling  5.123708\n",
      "2896       staff  5.173877\n",
      "2893         spy  5.175294\n",
      "3305       vague  5.180054\n",
      "2724     shallow  5.225538\n",
      "3198     trained  5.252605\n",
      "1020     engaged  5.253435\n",
      "2343       prime  5.256711\n",
      "811    described  5.258486\n",
      "232        avail  5.266733\n",
      "2614      rushed  5.270260\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer and transform the text data\n",
    "tfidf = TfidfVectorizer(max_features=3500)\n",
    "tfidf_matrix = tfidf.fit_transform(cleaned_corpus['text'])\n",
    "\n",
    "# Sum the TF-IDF scores for each term across all documents\n",
    "tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a DataFrame to store terms with their summed scores\n",
    "terms = tfidf.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame({'term': terms, 'score': tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame to get top 30 highest and lowest scoring terms\n",
    "top_30_highest = tfidf_df.nlargest(30, 'score')\n",
    "top_30_lowest = tfidf_df.nsmallest(30, 'score')\n",
    "\n",
    "print(top_30_highest)\n",
    "print(top_30_lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffe84d4-4aea-4d49-9529-08507b2cb7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: suicidal -> Top 5 co-occurence tokens: ['thought', 'know', 'im', 'ideation', 'feel']\n",
      "Term: depression -> Top 5 co-occurence tokens: ['anxiety', 'feel', 'year', 'life', 'im']\n",
      "Term: suicide -> Top 5 co-occurence tokens: ['cannot', 'hotline', 'commit', 'life', 'thought']\n",
      "Term: kill -> Top 5 co-occurence tokens: ['want', 'im', 'going', 'life', 'know']\n",
      "Term: myself -> Top 5 co-occurence tokens: ['kill', 'want', 'feel', 'pit', 'badly']\n",
      "Term: die -> Top 5 co-occurence tokens: ['want', 'im', 'life', 'live', 'know']\n",
      "Term: died -> Top 5 co-occurence tokens: ['dog', 'ago', 'year', 'life', 'dad']\n",
      "Term: pain -> Top 5 co-occurence tokens: ['want', 'life', 'im', 'cant', 'much']\n",
      "Term: sad -> Top 5 co-occurence tokens: ['feel', 'im', 'like', 'want', 'know']\n",
      "Term: help -> Top 5 co-occurence tokens: ['need', 'please', 'know', 'im', 'get']\n",
      "Term: sorry -> Top 5 co-occurence tokens: ['im', 'know', 'like', 'ive', 'time']\n",
      "Term: anxiety -> Top 5 co-occurence tokens: ['depression', 'social', 'im', 'ive', 'severe']\n",
      "Term: therapy -> Top 5 co-occurence tokens: ['therapist', 'medication', 'session', 'feel', 'ive']\n",
      "Term: suffering -> Top 5 co-occurence tokens: ['life', 'end', 'suffer', 'pain', 'want']\n",
      "Term: killing -> Top 5 co-occurence tokens: ['thinking', 'im', 'think', 'feel', 'ive']\n",
      "Term: pill -> Top 5 co-occurence tokens: ['sleeping', 'overdose', 'bottle', 'took', 'take']\n",
      "Term: movie -> Top 5 co-occurence tokens: ['character', 'great', 'scene', 'actor', 'film']\n",
      "Term: film -> Top 5 co-occurence tokens: ['scene', 'character', 'performance', 'director', 'movie']\n",
      "Term: character -> Top 5 co-occurence tokens: ['movie', 'film', 'plot', 'story', 'performance']\n",
      "Term: story -> Top 5 co-occurence tokens: ['movie', 'film', 'character', 'performance', 'great']\n",
      "Term: actor -> Top 5 co-occurence tokens: ['movie', 'film', 'role', 'character', 'performance']\n",
      "Term: performance -> Top 5 co-occurence tokens: ['film', 'role', 'character', 'movie', 'actor']\n",
      "Term: show -> Top 5 co-occurence tokens: ['television', 'episode', 'character', 'series', 'watch']\n",
      "Term: plot -> Top 5 co-occurence tokens: ['movie', 'character', 'film', 'twist', 'scene']\n",
      "Term: acting -> Top 5 co-occurence tokens: ['film', 'movie', 'great', 'character', 'actor']\n"
     ]
    }
   ],
   "source": [
    "# List of highlighted terms\n",
    "highlight_terms = ['suicidal', 'depression', 'suicide', 'kill', 'myself', 'die',\n",
    "                   'died', 'pain', 'sad', 'help', 'sorry', 'anxiety', 'therapy',\n",
    "                   'suffering', 'killing', 'pill',\n",
    "                   'movie', 'film', 'character', 'story', 'actor', 'performance', 'show',\n",
    "                   'plot', 'acting']\n",
    "\n",
    "# Create a dictionary to store the top 5 most similar tokens for each highlighted term\n",
    "highlight_similar_tokens = {}\n",
    "\n",
    "# Check if each highlighted term exists in the tokens and compute similarities if present\n",
    "for term in highlight_terms:\n",
    "    if term in tokens:\n",
    "        idx = list(tokens).index(term)\n",
    "        # Get similarities for the term with all other tokens\n",
    "        term_similarities = similarity_matrix[idx]\n",
    "\n",
    "        # Sort indices of similar tokens in descending order of similarity\n",
    "        similar_indices = term_similarities.argsort()[::-1]\n",
    "\n",
    "        # Select the top 5 tokens, skipping the first as it will be the term itself (similarity = 1.0)\n",
    "        top_5_similar = [tokens[i] for i in similar_indices[1:6]]\n",
    "        \n",
    "        # Store the top 5 similar tokens in the dictionary\n",
    "        highlight_similar_tokens[term] = top_5_similar\n",
    "    else:\n",
    "        highlight_similar_tokens[term] = [\"Token not found in vocabulary\"]\n",
    "\n",
    "# Display the results\n",
    "for term, similar_tokens in highlight_similar_tokens.items():\n",
    "    print(f\"Term: {term} -> Top 5 co-occurence tokens: {similar_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8690c1-5488-4dff-9df5-078efaa3e01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Document: Label 0: recently ive eating stomach problem im getting really bad stomach pain im hungry appetite throw little food eat think eating disorder something im sure\n",
      "Top 3 Similar Documents:\n",
      "1. Label 0: way im eating healthy im overweight want loose weight healthy able athletic thing stuff im overweight think whatever im eating still skinny people eat though might unknowingly eating little generally eat day breakfast small lunch maybe half cup leftover previous supper black tea honey let eat something unhealthy like piece chocolate supper eat little less regular serving whatever mom make ive week working time week havent expecting bad health effect dont think stomach lot thats probably normal someone calorie deficit maybe isnt idk need outsider opinion\n",
      "2. Label 0: dont want eat im reason im never bothered eating feel like im gaining weight never bothered motivation go get proper food sometimes im hungry eat cookie thats dont eat breakfast sometimes eat lunch sometimes dinner barely feel like eating\n",
      "3. Label 1: im hiding eating disorder boyfriend made account boyfriend know personal one anyways im year old ive issue body ever since suicidal felt like control whats going life time chose could control anything body fast metabolism already would eat usually ate sometimes throw food always count eat example im eating pizza count many olive slice pizza eat take forever eating never eat front people find weirdest excuse eat boyfriend person know eating disorder stopped smoking hope stop purging happy clean bad habit since april truth still suffering im afraid telling truth know tell hell start smoking fault know cant get rid easily thank whoever reply\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Sampled Document: Label 0: come rteenagers let people farm karma make post making fun karma get deleted karma ridiculous let meme ugh\n",
      "Top 3 Similar Documents:\n",
      "1. Label 0: get karma new acc cant get karma without posting cant post without karma\n",
      "2. Label 0: people idk comment karma time post comment karma post karma wow\n",
      "3. Label 0: whats karma mean im new site wtf karma like building something see people calling karma whore like people want bad brag pls supposed karma\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Sampled Document: Label 0: im true nerf war veteran im real even call pro use poop battle cum substitute bullet\n",
      "Top 3 Similar Documents:\n",
      "1. Label 0: know enemy know need fear result hundred battle finished art war sun everything war war deception ask anything\n",
      "2. Label 1: day lose battle day finally decide way return time decide like choice use resolve kill amount psychiatric help medication reverse decision lose battle lose battle nothing change mind point demon retain hopeful optimism consumed cynical nihilism defense today day lose battle im gonna go use\n",
      "3. Label 0: sister pro kam sister pro kam like barely talk dad men thats much hate men\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Sampled Document: Label 1: really want die also dont want leave anyone behind know wouldnt make really suicidal worse\n",
      "Top 3 Similar Documents:\n",
      "1. Label 0: hate award really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really hate award\n",
      "2. Label 1: want die want die want die want die want die want die want die want die want die want die want die want die want die want die want die want die want die die want die want die want die want die want die want die want die want die die want die want want die want die want die want die want die want die want want die want die die want die die\n",
      "3. Label 1: suicidal anyone want exactly die per make suicidal gesture end hospital like oding alcohol cutting really deep want thing badly dont necessarily want die dont know crave self destruction badly\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Sampled Document: Label 1: want anyone try help want die want die people give shit would fuckin life one decides die\n",
      "Top 3 Similar Documents:\n",
      "1. Label 1: want die want die want die want die want die want die want die want die want die want die want die want die want die want die want die want die want die die want die want die want die want die want die want die want die want die die want die want want die want die want die want die want die want die want want die want die die want die die\n",
      "2. Label 1: want die want die want die want die die im tire want fucking die\n",
      "3. Label 1: want kill escape feeling dont want die want die im probably going die\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Sampled Document: Label 1: even fucking know anymoremy grade failing relationship family abusive want live anymore keep trying best never work\n",
      "Top 3 Similar Documents:\n",
      "1. Label 1: even know anything anymoreim failing grade im also month behind school work family disappointed friend know\n",
      "2. Label 1: sitting parking lot wanting end allim really bad emotionally abusive relationship want leave cant guilt eating alive leave home cant think anything else ive driving around trying entertain trying entertain eating disorder mostly think much pain caused manipulative hows never tried help im embarrassed admit people relationship failing even parent really want son law cant handle pressure anymore day fine act anything less perfect hell make fun panic attack two week got left like didnt want deal want run car ramp im sure thatll kill completely dont want live like anymore much doubt hating dont know anymore\n",
      "3. Label 1: im disappointment im im currently freshman im failing two class mainly cant fucking concentrate work damn mental health bet parent would fucking disappointed got straight last quarter second quarter started im starting really want give cried twice earlier school grade feeling like shit ten time worse dont even want anymore killing would better teacher everyone think im bitch hurtful person cant even relationship without fucking considering huge fuck genuinely dont know anymore dont want keep going many thing mind fucking hate living much\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Filter documents by token length between 15 and 25\n",
    "filtered_docs = cleaned_corpus[\n",
    "    cleaned_corpus['text'].apply(lambda x: 15 <= len(x.split()) <= 25)\n",
    "]\n",
    "\n",
    "# Separate the filtered documents by class\n",
    "class_0_docs = filtered_docs[filtered_docs['label'] == 0]\n",
    "class_1_docs = filtered_docs[filtered_docs['label'] == 1]\n",
    "\n",
    "# Randomly select 3 documents from each class\n",
    "sample_class_0 = class_0_docs.sample(3, random_state=42)\n",
    "sample_class_1 = class_1_docs.sample(3, random_state=42)\n",
    "\n",
    "# Combine the samples\n",
    "sampled_docs = pd.concat([sample_class_0, sample_class_1])\n",
    "\n",
    "# Transform the text of all documents using the same TF-IDF vectorizer\n",
    "X_all = vectorizer.transform(cleaned_corpus['text'])\n",
    "\n",
    "# Calculate cosine similarity between the sampled documents and all documents\n",
    "similarity_matrix_all = cosine_similarity(X_all)\n",
    "\n",
    "# Store results in a dictionary\n",
    "similar_docs = {}\n",
    "\n",
    "# For each sampled document, find the top 3 most similar documents\n",
    "for idx, row in sampled_docs.iterrows():\n",
    "    # Get the similarity scores for the current document with all other documents\n",
    "    similarities = similarity_matrix_all[idx]\n",
    "\n",
    "    # Get the indices of the top 3 most similar documents (excluding the document itself)\n",
    "    similar_indices = similarities.argsort()[::-1][1:4]\n",
    "\n",
    "    # Store the results in the dictionary, including the label\n",
    "    similar_docs[f\"Label {row['label']}: {row['text']}\"] = [\n",
    "        f\"Label {cleaned_corpus.iloc[i]['label']}: {cleaned_corpus.iloc[i]['text']}\" \n",
    "        for i in similar_indices\n",
    "    ]\n",
    "\n",
    "# Display the results\n",
    "for doc, similar in similar_docs.items():\n",
    "    print(f\"Sampled Document: {doc}\")\n",
    "    print(\"Top 3 Similar Documents:\")\n",
    "    for i, sim_doc in enumerate(similar, start=1):\n",
    "        print(f\"{i}. {sim_doc}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ab51d50-2697-4006-a5ea-5aaf5e4e66e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar terms to 'suicidal':\n",
      "Term: thought, Similarity Score: 0.3086\n",
      "Term: know, Similarity Score: 0.1934\n",
      "Term: im, Similarity Score: 0.1856\n",
      "Term: ideation, Similarity Score: 0.1846\n",
      "Term: feel, Similarity Score: 0.1770\n",
      "\n",
      "Similar terms to 'depression':\n",
      "Term: anxiety, Similarity Score: 0.2359\n",
      "Term: feel, Similarity Score: 0.2037\n",
      "Term: year, Similarity Score: 0.1991\n",
      "Term: life, Similarity Score: 0.1908\n",
      "Term: im, Similarity Score: 0.1825\n",
      "\n",
      "Similar terms to 'suicide':\n",
      "Term: cannot, Similarity Score: 0.1240\n",
      "Term: hotline, Similarity Score: 0.0874\n",
      "Term: commit, Similarity Score: 0.0751\n",
      "Term: life, Similarity Score: 0.0658\n",
      "Term: thought, Similarity Score: 0.0654\n",
      "\n",
      "Similar terms to 'kill':\n",
      "Term: want, Similarity Score: 0.2724\n",
      "Term: im, Similarity Score: 0.2333\n",
      "Term: going, Similarity Score: 0.2116\n",
      "Term: life, Similarity Score: 0.2023\n",
      "Term: know, Similarity Score: 0.1876\n",
      "\n",
      "Similar terms to 'myself':\n",
      "Term: kill, Similarity Score: 0.1716\n",
      "Term: want, Similarity Score: 0.0752\n",
      "Term: feel, Similarity Score: 0.0587\n",
      "Term: pit, Similarity Score: 0.0536\n",
      "Term: badly, Similarity Score: 0.0526\n",
      "\n",
      "Similar terms to 'die':\n",
      "Term: want, Similarity Score: 0.3732\n",
      "Term: im, Similarity Score: 0.2223\n",
      "Term: life, Similarity Score: 0.2144\n",
      "Term: live, Similarity Score: 0.1964\n",
      "Term: know, Similarity Score: 0.1939\n",
      "\n",
      "Similar terms to 'died':\n",
      "Term: dog, Similarity Score: 0.1133\n",
      "Term: ago, Similarity Score: 0.1038\n",
      "Term: year, Similarity Score: 0.1002\n",
      "Term: life, Similarity Score: 0.0939\n",
      "Term: dad, Similarity Score: 0.0936\n",
      "\n",
      "Similar terms to 'pain':\n",
      "Term: want, Similarity Score: 0.1990\n",
      "Term: life, Similarity Score: 0.1848\n",
      "Term: im, Similarity Score: 0.1732\n",
      "Term: cant, Similarity Score: 0.1652\n",
      "Term: much, Similarity Score: 0.1639\n",
      "\n",
      "Similar terms to 'sad':\n",
      "Term: feel, Similarity Score: 0.1789\n",
      "Term: im, Similarity Score: 0.1512\n",
      "Term: like, Similarity Score: 0.1400\n",
      "Term: want, Similarity Score: 0.1337\n",
      "Term: know, Similarity Score: 0.1280\n",
      "\n",
      "Similar terms to 'help':\n",
      "Term: need, Similarity Score: 0.3198\n",
      "Term: please, Similarity Score: 0.2666\n",
      "Term: know, Similarity Score: 0.2607\n",
      "Term: im, Similarity Score: 0.2446\n",
      "Term: get, Similarity Score: 0.2267\n",
      "\n",
      "Similar terms to 'sorry':\n",
      "Term: im, Similarity Score: 0.2300\n",
      "Term: know, Similarity Score: 0.1450\n",
      "Term: like, Similarity Score: 0.1172\n",
      "Term: ive, Similarity Score: 0.1157\n",
      "Term: time, Similarity Score: 0.1152\n",
      "\n",
      "Similar terms to 'anxiety':\n",
      "Term: depression, Similarity Score: 0.2359\n",
      "Term: social, Similarity Score: 0.2052\n",
      "Term: im, Similarity Score: 0.1766\n",
      "Term: ive, Similarity Score: 0.1543\n",
      "Term: severe, Similarity Score: 0.1538\n",
      "\n",
      "Similar terms to 'therapy':\n",
      "Term: therapist, Similarity Score: 0.1736\n",
      "Term: medication, Similarity Score: 0.1677\n",
      "Term: session, Similarity Score: 0.1653\n",
      "Term: feel, Similarity Score: 0.1380\n",
      "Term: ive, Similarity Score: 0.1339\n",
      "\n",
      "Similar terms to 'suffering':\n",
      "Term: life, Similarity Score: 0.1341\n",
      "Term: end, Similarity Score: 0.1217\n",
      "Term: suffer, Similarity Score: 0.1212\n",
      "Term: pain, Similarity Score: 0.1149\n",
      "Term: want, Similarity Score: 0.1098\n",
      "\n",
      "Similar terms to 'killing':\n",
      "Term: thinking, Similarity Score: 0.1526\n",
      "Term: im, Similarity Score: 0.1445\n",
      "Term: think, Similarity Score: 0.1280\n",
      "Term: feel, Similarity Score: 0.1239\n",
      "Term: ive, Similarity Score: 0.1152\n",
      "\n",
      "Similar terms to 'pill':\n",
      "Term: sleeping, Similarity Score: 0.1648\n",
      "Term: overdose, Similarity Score: 0.1647\n",
      "Term: bottle, Similarity Score: 0.1606\n",
      "Term: took, Similarity Score: 0.1476\n",
      "Term: take, Similarity Score: 0.1331\n",
      "\n",
      "Similar terms to 'movie':\n",
      "Term: character, Similarity Score: 0.2840\n",
      "Term: great, Similarity Score: 0.2632\n",
      "Term: scene, Similarity Score: 0.2584\n",
      "Term: actor, Similarity Score: 0.2575\n",
      "Term: film, Similarity Score: 0.2391\n",
      "\n",
      "Similar terms to 'film':\n",
      "Term: scene, Similarity Score: 0.3011\n",
      "Term: character, Similarity Score: 0.2833\n",
      "Term: performance, Similarity Score: 0.2784\n",
      "Term: director, Similarity Score: 0.2461\n",
      "Term: movie, Similarity Score: 0.2391\n",
      "\n",
      "Similar terms to 'character':\n",
      "Term: movie, Similarity Score: 0.2840\n",
      "Term: film, Similarity Score: 0.2833\n",
      "Term: plot, Similarity Score: 0.2175\n",
      "Term: story, Similarity Score: 0.2069\n",
      "Term: performance, Similarity Score: 0.2008\n",
      "\n",
      "Similar terms to 'story':\n",
      "Term: movie, Similarity Score: 0.2390\n",
      "Term: film, Similarity Score: 0.2292\n",
      "Term: character, Similarity Score: 0.2069\n",
      "Term: performance, Similarity Score: 0.1531\n",
      "Term: great, Similarity Score: 0.1509\n",
      "\n",
      "Similar terms to 'actor':\n",
      "Term: movie, Similarity Score: 0.2575\n",
      "Term: film, Similarity Score: 0.2372\n",
      "Term: role, Similarity Score: 0.2080\n",
      "Term: character, Similarity Score: 0.1978\n",
      "Term: performance, Similarity Score: 0.1972\n",
      "\n",
      "Similar terms to 'performance':\n",
      "Term: film, Similarity Score: 0.2784\n",
      "Term: role, Similarity Score: 0.2058\n",
      "Term: character, Similarity Score: 0.2008\n",
      "Term: movie, Similarity Score: 0.1975\n",
      "Term: actor, Similarity Score: 0.1972\n",
      "\n",
      "Similar terms to 'show':\n",
      "Term: television, Similarity Score: 0.1753\n",
      "Term: episode, Similarity Score: 0.1542\n",
      "Term: character, Similarity Score: 0.1485\n",
      "Term: series, Similarity Score: 0.1324\n",
      "Term: watch, Similarity Score: 0.1276\n",
      "\n",
      "Similar terms to 'plot':\n",
      "Term: movie, Similarity Score: 0.2296\n",
      "Term: character, Similarity Score: 0.2175\n",
      "Term: film, Similarity Score: 0.2127\n",
      "Term: twist, Similarity Score: 0.1539\n",
      "Term: scene, Similarity Score: 0.1381\n",
      "\n",
      "Similar terms to 'acting':\n",
      "Term: film, Similarity Score: 0.2173\n",
      "Term: movie, Similarity Score: 0.2160\n",
      "Term: great, Similarity Score: 0.1567\n",
      "Term: character, Similarity Score: 0.1543\n",
      "Term: actor, Similarity Score: 0.1437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned corpus\n",
    "cleaned_corpus = pd.read_csv('data/cleaned_mhc.csv')\n",
    "\n",
    "# Initialize the TF-IDF vectorizer with a maximum of 3500 features\n",
    "vectorizer = TfidfVectorizer(max_features=3500)\n",
    "\n",
    "# Fit and transform the text data to create the TF-IDF matrix\n",
    "X_tfidf = vectorizer.fit_transform(cleaned_corpus['text'])\n",
    "\n",
    "# Apply Truncated SVD for LSA\n",
    "n_components = 100  # You can adjust this based on your data\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the SVD model and transform the TF-IDF matrix\n",
    "X_lsa = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# Compute cosine similarity for terms (features)\n",
    "cosine_similarities_terms = cosine_similarity(X_tfidf.T)\n",
    "\n",
    "# Highlighted terms to find similar terms for\n",
    "highlight_terms = ['suicidal', 'depression', 'suicide', 'kill', 'myself', 'die',\n",
    "                   'died', 'pain', 'sad', 'help', 'sorry', 'anxiety', 'therapy',\n",
    "                   'suffering', 'killing', 'pill',\n",
    "                   'movie', 'film', 'character', 'story', 'actor', 'performance', 'show',\n",
    "                   'plot', 'acting']\n",
    "\n",
    "# Function to display similar terms for highlighted terms\n",
    "def display_highlighted_terms(highlight_terms, top_n=5):\n",
    "    for term in highlight_terms:\n",
    "        if term in vectorizer.get_feature_names_out():\n",
    "            term_index = vectorizer.get_feature_names_out().tolist().index(term)\n",
    "            print(f\"\\nSimilar terms to '{term}':\")\n",
    "            similar_indices = np.argsort(cosine_similarities_terms[term_index])[::-1][1:top_n+1]\n",
    "            for i in similar_indices:\n",
    "                print(f\"Term: {vectorizer.get_feature_names_out()[i]}, Similarity Score: {cosine_similarities_terms[term_index][i]:.4f}\")\n",
    "        else:\n",
    "            print(f\"Term '{term}' not found in the TF-IDF features.\")\n",
    "\n",
    "# Display similar terms for the specified highlight terms\n",
    "display_highlighted_terms(highlight_terms, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9010b0d-fba1-4883-92a1-1fa5c3db9385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
