{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246a8a1b-f1a8-4afc-8e5f-07e7305c6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nhập thư viện\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60a249b-3ffb-4f0e-ad38-71b131e5e57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:\n",
      "                                                 text  label\n",
      "0  dear american teens question dutch person hear...      0\n",
      "1  nothing look forward lifei dont many reasons k...      1\n",
      "2  music recommendations im looking expand playli...      0\n",
      "3  im done trying feel betterthe reason im still ...      1\n",
      "4  worried  year old girl subject domestic physic...      1\n",
      "5  hey rredflag sure right place post this goes  ...      1\n",
      "6  feel like someone needs hear tonight feeling r...      0\n",
      "7  deserve liveif died right noone would carei re...      1\n",
      "8  feels good ive set dateim killing friday nice ...      1\n",
      "9  live guiltok made stupid random choice  its ge...      1 \n",
      "\n",
      "Cleaned Corpus:\n",
      "                                                 text  label\n",
      "0  dear american teen question dutch person heard...      0\n",
      "1  nothing look forward life dont many reason kee...      1\n",
      "2  music recommendation im looking expand playlis...      0\n",
      "3  im done trying feel reason im still alive know...      1\n",
      "4  worried year old girl subject domestic going l...      1\n",
      "5  hey rredflag sure right place post go im curre...      1\n",
      "6  feel like someone need hear tonight feeling ri...      0\n",
      "7  deserve died right noone would care real frien...      1\n",
      "8  feel good ive set killing friday nice finally ...      1\n",
      "9  live made stupid random choice getting basical...      1\n",
      "\n",
      "Shape of original corpus: (27977, 2) and cleaned corpus: (27940, 2)\n",
      "Sample difference: 37\n"
     ]
    }
   ],
   "source": [
    "# Đọc 2 bộ dự liệu để so sánh\n",
    "org_corpus = pd.read_csv('data/mental_health.csv')\n",
    "\n",
    "cln_corpus = pd.read_csv('data/cleaned_mhc.csv')\n",
    "\n",
    "print(\"Original Corpus:\\n\", org_corpus.head(10), \"\\n\")\n",
    "print(\"Cleaned Corpus:\\n\", cln_corpus.head(10))\n",
    "\n",
    "print(f\"\\nShape of original corpus: {org_corpus.shape} and cleaned corpus: {cln_corpus.shape}\")\n",
    "print(f\"Sample difference: {org_corpus.shape[0] - cln_corpus.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c204232-ef5b-44fd-aa95-f54640e6bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus Stats:\n",
      "Class 1: Total tokens = 1337600, Unique tokens = 42130\n",
      "Class 0: Total tokens = 670013, Unique tokens = 49022\n",
      "Entire Corpus: Total tokens = 2007613, Unique tokens = 72649\n",
      "\n",
      "Cleaned Corpus Stats:\n",
      "Class 1: Total tokens = 1212239, Unique tokens = 12516\n",
      "Class 0: Total tokens = 584055, Unique tokens = 13883\n",
      "Entire Corpus: Total tokens = 1796294, Unique tokens = 14599\n"
     ]
    }
   ],
   "source": [
    "# So sánh unique tokens và total tokens\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def token_stats(corpus, label_column='label'):\n",
    "    total_tokens_class_1 = corpus[corpus[label_column] == 1]['text'].apply(lambda x: len(tokenize(x))).sum()\n",
    "    total_tokens_class_0 = corpus[corpus[label_column] == 0]['text'].apply(lambda x: len(tokenize(x))).sum()\n",
    "    \n",
    "    unique_tokens_class_1 = set(token for text in corpus[corpus[label_column] == 1]['text'] for token in tokenize(text))\n",
    "    unique_tokens_class_0 = set(token for text in corpus[corpus[label_column] == 0]['text'] for token in tokenize(text))\n",
    "\n",
    "    total_tokens_entire = corpus['text'].apply(lambda x: len(tokenize(x))).sum()\n",
    "    unique_tokens_entire = set(token for text in corpus['text'] for token in tokenize(text))\n",
    "\n",
    "    return {\n",
    "        'total_tokens_class_1': total_tokens_class_1,\n",
    "        'total_tokens_class_0': total_tokens_class_0,\n",
    "        'unique_tokens_class_1': len(unique_tokens_class_1),\n",
    "        'unique_tokens_class_0': len(unique_tokens_class_0),\n",
    "        'total_tokens_entire': total_tokens_entire,\n",
    "        'unique_tokens_entire': len(unique_tokens_entire)\n",
    "    }\n",
    "\n",
    "original_stats = token_stats(org_corpus)\n",
    "\n",
    "cleaned_stats = token_stats(cln_corpus)\n",
    "\n",
    "print(\"Original Corpus Stats:\")\n",
    "print(f\"Class 1: Total tokens = {original_stats['total_tokens_class_1']}, Unique tokens = {original_stats['unique_tokens_class_1']}\")\n",
    "print(f\"Class 0: Total tokens = {original_stats['total_tokens_class_0']}, Unique tokens = {original_stats['unique_tokens_class_0']}\")\n",
    "print(f\"Entire Corpus: Total tokens = {original_stats['total_tokens_entire']}, Unique tokens = {original_stats['unique_tokens_entire']}\\n\")\n",
    "\n",
    "print(\"Cleaned Corpus Stats:\")\n",
    "print(f\"Class 1: Total tokens = {cleaned_stats['total_tokens_class_1']}, Unique tokens = {cleaned_stats['unique_tokens_class_1']}\")\n",
    "print(f\"Class 0: Total tokens = {cleaned_stats['total_tokens_class_0']}, Unique tokens = {cleaned_stats['unique_tokens_class_0']}\")\n",
    "print(f\"Entire Corpus: Total tokens = {cleaned_stats['total_tokens_entire']}, Unique tokens = {cleaned_stats['unique_tokens_entire']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74ab355-8eb7-4193-a8f0-a6fdb9c2c4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus Class Distribution:\n",
      "Class 0: Count = 14139, Percentage = 50.5%\n",
      "Class 1: Count = 13838, Percentage = 49.5%\n",
      "\n",
      "Cleaned Corpus Class Distribution:\n",
      "Class 0: Count = 14122, Percentage = 50.5%\n",
      "Class 1: Count = 13818, Percentage = 49.5%\n"
     ]
    }
   ],
   "source": [
    "# So sánh sự khác nhau của phân bổ lớp\n",
    "def class_distribution_with_percentage(corpus, label_column='label'):\n",
    "    class_counts = corpus[label_column].value_counts()\n",
    "    total_count = class_counts.sum()\n",
    "    percentages = (class_counts / total_count) * 100\n",
    "    return class_counts, percentages\n",
    "\n",
    "original_counts, original_percentages = class_distribution_with_percentage(org_corpus)\n",
    "\n",
    "cleaned_counts, cleaned_percentages = class_distribution_with_percentage(cln_corpus)\n",
    "\n",
    "print(\"Original Corpus Class Distribution:\")\n",
    "for label in original_counts.index:\n",
    "    print(f\"Class {label}: Count = {original_counts[label]}, Percentage = {original_percentages[label]:.1f}%\")\n",
    "\n",
    "print(\"\\nCleaned Corpus Class Distribution:\")\n",
    "for label in cleaned_counts.index:\n",
    "    print(f\"Class {label}: Count = {cleaned_counts[label]}, Percentage = {cleaned_percentages[label]:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "012895b7-3004-43dc-a1cd-bd6c65a5245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN on Original Corpus with TF-IDF:\n",
      "Epoch 1/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 109ms/step - accuracy: 0.8243 - loss: 0.3702\n",
      "Epoch 2/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 103ms/step - accuracy: 0.9288 - loss: 0.1807\n",
      "Epoch 3/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 104ms/step - accuracy: 0.9520 - loss: 0.1194\n",
      "Epoch 4/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 117ms/step - accuracy: 0.9739 - loss: 0.0717\n",
      "Epoch 5/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 111ms/step - accuracy: 0.9875 - loss: 0.0350\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      7126\n",
      "           1       0.90      0.87      0.89      6863\n",
      "\n",
      "    accuracy                           0.89     13989\n",
      "   macro avg       0.89      0.89      0.89     13989\n",
      "weighted avg       0.89      0.89      0.89     13989\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6454  672]\n",
      " [ 881 5982]]\n",
      "\n",
      "CNN on Cleaned Corpus with TF-IDF:\n",
      "Epoch 1/5\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 104ms/step - accuracy: 0.7929 - loss: 0.4375\n",
      "Epoch 2/5\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 104ms/step - accuracy: 0.9252 - loss: 0.1870\n",
      "Epoch 3/5\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 101ms/step - accuracy: 0.9510 - loss: 0.1224\n",
      "Epoch 4/5\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 102ms/step - accuracy: 0.9758 - loss: 0.0641\n",
      "Epoch 5/5\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 121ms/step - accuracy: 0.9902 - loss: 0.0289\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      7065\n",
      "           1       0.89      0.89      0.89      6905\n",
      "\n",
      "    accuracy                           0.89     13970\n",
      "   macro avg       0.89      0.89      0.89     13970\n",
      "weighted avg       0.89      0.89      0.89     13970\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6330  735]\n",
      " [ 788 6117]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the original and cleaned corpora\n",
    "org_corpus = pd.read_csv('data/mental_health.csv')\n",
    "cln_corpus = pd.read_csv('data/cleaned_mhc.csv')\n",
    "\n",
    "def build_cnn_model(input_dim):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(input_dim, 1)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification output\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluate_cnn_tfidf(corpus):\n",
    "    X = corpus['text']\n",
    "    y = corpus['label']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=2500)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # Normalize the TF-IDF features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "    X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "    # Reshape data for CNN (samples, features, channels)\n",
    "    X_train_tfidf = X_train_tfidf.reshape(X_train_tfidf.shape[0], X_train_tfidf.shape[1], 1)\n",
    "    X_test_tfidf = X_test_tfidf.reshape(X_test_tfidf.shape[0], X_test_tfidf.shape[1], 1)\n",
    "\n",
    "    # Build the CNN model\n",
    "    model = build_cnn_model(input_dim=X_train_tfidf.shape[1])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_tfidf, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = (model.predict(X_test_tfidf) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluate CNN on the original corpus with TF-IDF\n",
    "print(\"CNN on Original Corpus with TF-IDF:\")\n",
    "evaluate_cnn_tfidf(org_corpus)\n",
    "\n",
    "# Evaluate CNN on the cleaned corpus with TF-IDF\n",
    "print(\"\\nCNN on Cleaned Corpus with TF-IDF:\")\n",
    "evaluate_cnn_tfidf(cln_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
