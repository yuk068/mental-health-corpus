{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d063797b-b39e-4ab6-9d95-65c03bb3e1b7",
   "metadata": {},
   "source": [
    "# Dọn dữ liệu\n",
    "\n",
    "- Từ cơ sở ở phần phân tích bộ Corpus đã xử lý.\n",
    "- Mục đích của phần này là từ bộ Corpus đã xử lý `./data/mental_health.csv`, ta thu được `./data/cleaned_mhc.csv` đã được dọn.\n",
    "- Việc dọn dữ liệu sẽ giúp chúng ta:\n",
    "    - Giảm không gian đặc trưng, nhiễu của bộ dữ liệu mà không mất mát thông tin quá lớn.\n",
    "    - Bộ dữ liệu được khách quan hơn và phản ánh chính xác hơn về chủ đề được nói đến.\n",
    "    - Giúp cải thiện hoặc giữ nguyên kết quả của các thuật toán học máy và giảm lượng tính toán cần thiết.\n",
    "\n",
    "# Các bước dọn bộ Corpus đã xử lý\n",
    "\n",
    "1. Lemmatization và loại bỏ Stop-words\n",
    "2. Loại bỏ các tokens dài hơn 17 ký tự.\n",
    "3. Xóa tất cả ký tự 'i' ở cuối tại mọi tokens kết thúc bằng ký tự 'i'.\n",
    "4. Đưa những yếu tố có char lặp lại về 1 char (eg. soooooo -> so, dddddd -> d).\n",
    "5. Loại bỏ các tokens cụ thể trên cơ sở của phần phân tích n-Grams:\n",
    "- filler\n",
    "- br\n",
    "- nowdrink\n",
    "- pee\n",
    "- poo\n",
    "- eve\n",
    "- click\n",
    "- horny\n",
    "- ampxb\n",
    "- monitor\n",
    "- Mọi tokens bắt đầu bằng 'gt'\n",
    "\n",
    "6. Ngoại trừ Tokens \\['im' 'go', 'do'], loại bỏ mọi Tokens có độ dài nhỏ hơn 3.\n",
    "7. Loại bỏ các token có tần suất nhỏ hơn 5.\n",
    "8. Chuẩn hóa khoảng trắng.\n",
    "9. Chỉ giữ lại các mẫu có độ dài từ 10 - 400 Tokens.\n",
    "10. Loại bỏ các mẫu trống hoặc chỉ có khoảng trắng.\n",
    "11. Loại bỏ các mẫu trùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a443fe4-d5a0-4a62-9bba-64829a8ebd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nhập thư viện\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Đọc bộ dữ liệu\n",
    "corpus = pd.read_csv('data/processed_mhc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99460c63-2589-4f2e-9696-7e8ff30980c7",
   "metadata": {},
   "source": [
    "### 1. Lemmatization và loại bỏ Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b08c41-955a-4296-9275-f74eeff62290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected by lemmatization: 22841\n",
      "Total number of rows affected by stop words removal: 19444\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'] = corpus['text'].apply(lemmatize_text)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "print(f\"Total number of rows affected by lemmatization: {affected_rows}\")\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'] = corpus['text'].apply(remove_stop_words)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "print(f\"Total number of rows affected by stop words removal: {affected_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecada77-7978-4287-95d4-046818340c9c",
   "metadata": {},
   "source": [
    "### 2. Loại bỏ các tokens dài hơn 17 ký tự"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b086f3-6af5-4140-ab59-f6d089207dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with tokens longer than 17 characters: 1450\n",
      "Number of rows with tokens longer than 17 characters after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "def remove_long_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if len(token) <= 17]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "rows_with_long_tokens = corpus[corpus['text'].str.contains(r'\\b\\w{18,}\\b', regex=True)]\n",
    "print(f\"Total rows with tokens longer than 17 characters: {rows_with_long_tokens.shape[0]}\")\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(remove_long_tokens)\n",
    "\n",
    "rows_with_long_tokens_after = corpus[corpus['text'].str.contains(r'\\b\\w{18,}\\b', regex=True)]\n",
    "print(f\"Number of rows with tokens longer than 17 characters after cleaning: {rows_with_long_tokens_after.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371c50c-8aac-43d7-afb4-0570b92b7f65",
   "metadata": {},
   "source": [
    "### 3. Xóa tất cả ký tự 'i' ở cuối tại mọi tokens kết thúc bằng ký tự 'i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfeea40-8e09-446a-94e9-18bedf5eddf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ending with 'i' before cleaning:\n",
      "lifei\n",
      "carei\n",
      "friendsi\n",
      "usi\n",
      "siblingsi\n",
      "oni\n",
      "ii\n",
      "upi\n",
      "chapteri\n",
      "oni\n",
      "bi\n",
      "bi\n",
      "illuminati\n",
      "illuminati\n",
      "tomorrowi\n",
      "\n",
      "Number of rows with tokens containing trailing 'i' after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "def find_tokens_ending_with_i(text):\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens if token.endswith('i')]\n",
    "\n",
    "tokens_before_cleaning = [token for text in corpus['text'] for token in find_tokens_ending_with_i(text)]\n",
    "\n",
    "print(\"Tokens ending with 'i' before cleaning:\")\n",
    "for token in tokens_before_cleaning[:15]:\n",
    "    print(token)\n",
    "\n",
    "def remove_trailing_i_properly(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [re.sub(r'i+$', '', token) for token in tokens]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(remove_trailing_i_properly)\n",
    "\n",
    "rows_with_trailing_i_after = corpus[corpus['text'].str.contains(r'\\b\\w*i\\b')]\n",
    "print(f\"\\nNumber of rows with tokens containing trailing 'i' after cleaning: {rows_with_trailing_i_after.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c143d7-876d-4303-9e55-ca1ada65f23e",
   "metadata": {},
   "source": [
    "### 4. Đưa những yếu tố có char lặp lại về 1 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793b7783-5cb6-4483-b01a-8f193f7caf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total changes made in repeated character handling: 192147\n",
      "Total remaining instances with repeated characters: 0\n"
     ]
    }
   ],
   "source": [
    "def handle_repeated_chars(text):\n",
    "    cleaned_text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    changes_made = sum(1 for original, cleaned in zip(text, cleaned_text) if original != cleaned)\n",
    "    return cleaned_text, changes_made\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "\n",
    "corpus['text'], changes_made = zip(*corpus['text'].apply(handle_repeated_chars))\n",
    "\n",
    "total_changes = sum(changes_made)\n",
    "\n",
    "remaining_instances = corpus[corpus['text'].str.contains(r'(.)\\1{2,}', regex=True)]\n",
    "\n",
    "print(f\"Total changes made in repeated character handling: {total_changes}\")\n",
    "print(f\"Total remaining instances with repeated characters: {remaining_instances.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b7115-933d-4b10-876c-14909ec10d84",
   "metadata": {},
   "source": [
    "### 5. Loại bỏ các tokens cụ thể trên cơ sở của phần phân tích n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19eea862-d6bd-465c-badc-e86df44813bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected: 2521\n",
      "Instances found of specified words: 0\n",
      "Instances found of gt-prefixed words: 0\n"
     ]
    }
   ],
   "source": [
    "specific_words = ['filler', 'br', 'eve', 'pee', 'poo', 'ampxb', 'nowdrink', 'monitor', 'click', 'horny']\n",
    "\n",
    "def remove_specific_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if token not in specific_words and not token.startswith('gt')]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(remove_specific_tokens)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "\n",
    "print(f\"Total number of rows affected: {affected_rows}\")\n",
    "\n",
    "def verify_removal(corpus, specific_words, column='text'):\n",
    "    word_counts = {word: corpus[column].str.count(fr'\\b{word}\\b').sum() for word in specific_words}\n",
    "    \n",
    "    gt_count = corpus[column].str.count(r'\\bgt\\w+').sum()\n",
    "    \n",
    "    print(f\"Instances found of specified words: {sum(word_counts.values())}\")\n",
    "    print(f\"Instances found of gt-prefixed words: {gt_count}\")\n",
    "    \n",
    "    if sum(word_counts.values()) > 0:\n",
    "        for word, count in word_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"- '{word}': {count}\")\n",
    "\n",
    "verify_removal(corpus, specific_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bce317-4345-4c02-b61d-752fc3b99162",
   "metadata": {},
   "source": [
    "### 6. Ngoại trừ Tokens \\['im' 'go', 'do'], loại bỏ mọi Tokens có độ dài nhỏ hơn 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e480e9f-cb52-4f38-9a24-ce4873a9202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected: 10853\n",
      "Total remaining instances with tokens of 2 or less characters (excluding ['im', 'go', 'do']): 0\n"
     ]
    }
   ],
   "source": [
    "specific_words_to_keep = ['im', 'go', 'do']\n",
    "\n",
    "def remove_tokens_by_length(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if len(token) > 2 or token in specific_words_to_keep]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "\n",
    "# Apply the cleaning process\n",
    "corpus['text'] = corpus['text'].apply(remove_tokens_by_length)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "\n",
    "print(f\"Total number of rows affected: {affected_rows}\")\n",
    "\n",
    "remaining_instances = corpus[corpus['text'].str.split().apply(lambda x: any(len(token) <= 2 and token not in specific_words_to_keep for token in x))]\n",
    "\n",
    "remaining_count = remaining_instances.shape[0]\n",
    "print(f\"Total remaining instances with tokens of 2 or less characters (excluding {specific_words_to_keep}): {remaining_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd09d95-a851-4a50-a202-3420c42bdc6a",
   "metadata": {},
   "source": [
    "### 7. Loại bỏ các token có tần suất nhỏ hơn 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3991fb-03da-4580-aa73-36612ff108e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected: 17473\n",
      "Remaining low-frequency tokens after removal: []\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [token for text in corpus['text'] for token in text.split()]\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "tokens_to_remove = {token for token, freq in token_freq.items() if freq <= 4}\n",
    "\n",
    "def remove_low_freq_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if token not in tokens_to_remove]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'] = corpus['text'].apply(remove_low_freq_tokens)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "\n",
    "print(f\"Total number of rows affected: {affected_rows}\")\n",
    "\n",
    "remaining_tokens = [token for text in corpus['text'] for token in text.split() if token in tokens_to_remove]\n",
    "\n",
    "print(f\"Remaining low-frequency tokens after removal: {remaining_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062abe99-5ab2-4569-9b35-677e135c003d",
   "metadata": {},
   "source": [
    "### 8. Chuẩn hóa khoảng trắng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec095f04-c7fb-4754-a143-27bd344b373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows with multiple spaces, tabs, or newlines: 0\n",
      "Number of rows with multiple spaces, tabs, or newlines after normalization: 0\n"
     ]
    }
   ],
   "source": [
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "rows_with_extra_whitespace = corpus[corpus['text'].str.contains(r'\\s{2,}|\\t|\\n', regex=True)]\n",
    "print(f\"Total number of rows with multiple spaces, tabs, or newlines: {rows_with_extra_whitespace.shape[0]}\")\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(normalize_whitespace)\n",
    "\n",
    "remaining_whitespace_rows = corpus[corpus['text'].str.contains(r'\\s{2,}|\\t|\\n', regex=True)]\n",
    "print(f\"Number of rows with multiple spaces, tabs, or newlines after normalization: {remaining_whitespace_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3389d8-04f2-4de4-a8c4-0380c6b9f4ae",
   "metadata": {},
   "source": [
    "### 9. Chỉ giữ các mẫu có Total Tokens trong khoảng 10 - 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57821cf-d537-4c42-9df4-ceb7937b3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples before filtering: 27969\n",
      "Number of samples after filtering: 23240\n"
     ]
    }
   ],
   "source": [
    "num_samples_before = corpus.shape[0]\n",
    "print(f\"Number of samples before filtering: {num_samples_before}\")\n",
    "\n",
    "corpus = corpus[corpus['text'].apply(lambda text: len(text.split()) >= 10 and len(text.split()) <= 400)]\n",
    "\n",
    "num_samples_after = corpus.shape[0]\n",
    "print(f\"Number of samples after filtering: {num_samples_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ff55b-2e41-4c6a-b769-3b5bf80e274a",
   "metadata": {},
   "source": [
    "### 10. Loại bỏ các điểm trống hoặc chỉ có khoảng trắng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175573ea-acd0-4a7c-bb47-00ee15e7144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before removing empty or whitespace-only rows: 23240\n",
      "Total rows after removing empty or whitespace-only rows: 23240\n",
      "Total rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "total_rows_before = corpus.shape[0]\n",
    "print(f\"Total rows before removing empty or whitespace-only rows: {total_rows_before}\")\n",
    "\n",
    "corpus = corpus[corpus['text'].str.strip().astype(bool)]\n",
    "\n",
    "total_rows_after = corpus.shape[0]\n",
    "print(f\"Total rows after removing empty or whitespace-only rows: {total_rows_after}\")\n",
    "\n",
    "rows_removed = total_rows_before - total_rows_after\n",
    "print(f\"Total rows removed: {rows_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9909a-50c3-40f0-b898-ac5c11220cb1",
   "metadata": {},
   "source": [
    "### 11. Loại bỏ các điểm trùng nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e605a846-c402-4aa1-95f7-b24fcfc57e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before removing duplicates: 23240\n",
      "Indices of duplicate rows:\n",
      "[]\n",
      "\n",
      "Content of duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [text, label]\n",
      "Index: []\n",
      "\n",
      "Total rows after removing duplicates: 23240\n",
      "Number of rows removed: 0\n",
      "Number of duplicate rows after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "before_drop_dupes = corpus.shape[0]\n",
    "print(f\"Total rows before removing duplicates: {before_drop_dupes}\")\n",
    "\n",
    "duplicates = corpus[corpus.duplicated(subset=['text'], keep=False)]\n",
    "\n",
    "print(\"Indices of duplicate rows:\")\n",
    "print(duplicates.index.tolist())\n",
    "print(\"\\nContent of duplicate rows:\")\n",
    "print(duplicates.head(len(corpus.duplicated(subset=['text'], keep=False))))\n",
    "\n",
    "corpus = corpus.drop_duplicates(subset=['text'], keep='first')\n",
    "\n",
    "print(f\"\\nTotal rows after removing duplicates: {corpus.shape[0]}\")\n",
    "\n",
    "rows_removed = before_drop_dupes - corpus.shape[0]\n",
    "print(f\"Number of rows removed: {rows_removed}\")\n",
    "\n",
    "remaining_duplicates = corpus.duplicated(subset=['text']).sum()\n",
    "print(f\"Number of duplicate rows after cleaning: {remaining_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f250c71-a734-4ac6-80a3-0ce39fbcc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu lại `data/cleaned_mhc.csv`\n",
    "corpus.to_csv('data/cleaned_mhc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
