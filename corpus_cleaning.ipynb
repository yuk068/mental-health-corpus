{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb7ac5b-1c9e-45fb-947b-972b6a2e80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nhập thư viện\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('data/mental_health.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997daec-d140-4193-b612-c5e6b87856f7",
   "metadata": {},
   "source": [
    "# Tổng quát về quá trình dọn dữ liệu\n",
    "\n",
    "### Quy trình:\n",
    "1. Đảm bảo các văn bản chỉ chứa các chữ cái viết thường trong ngôn ngữ Tiếng Anh.\n",
    "2. Loại bỏ các đường link (token có chứa http, www).\n",
    "3. Loại bỏ các tokens dài hơn 16 ký tự.\n",
    "4. Xóa ký tự 'i' ở cuối tại mọi tokens kết thúc bằng ký tự 'i'\n",
    "5. Loại bỏ các tokens cụ thể trên cơ sở của phần phân tích n-Grams:\n",
    "- filler\n",
    "- br\n",
    "- nowdrink\n",
    "- pee\n",
    "- poo\n",
    "- eve\n",
    "- click\n",
    "- horny\n",
    "- ampxb\n",
    "- monitor\n",
    "- Mọi tokens bắt đầu bằng 'gt'\n",
    "\n",
    "6. Thực hiện Lemmatization.\n",
    "7. Thực hiện Stop-words removal.\n",
    "8. Đưa những yếu tố có char lặp lại về 1 char (eg. soooooo -> so, dddddd -> d).\n",
    "9. Loại bỏ các token chỉ có 1 char.\n",
    "10. Loại bỏ các token có tần suất nhỏ hơn 5.\n",
    "11. Loại bỏ các sample được coi là dị thường dựa vào total tokens.\n",
    "12. Loại bỏ các sample trống, chỉ còn whitespace.\n",
    "\n",
    "#### Lưu ý: Quy trình này được lặp lại cho tới khi không còn thay đổi nào được thực hiện (4 lần)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fb77732-7b6b-43af-8bbe-4007b6430896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters removed: set()\n"
     ]
    }
   ],
   "source": [
    "# Bước 1: Đảm bảo các văn bản chỉ chứa các chữ cái viết thường trong ngôn ngữ Tiếng Anh\n",
    "def remove_non_lowercase(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    removed_chars = set(re.findall(r'[^a-z\\s]', text))\n",
    "    cleaned_text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return cleaned_text.strip(), removed_chars\n",
    "\n",
    "corpus['text'], removed_chars = zip(*corpus['text'].apply(remove_non_lowercase))\n",
    "\n",
    "removed_chars = set(char for chars in removed_chars for char in chars)\n",
    "\n",
    "print(\"Unique characters removed:\", removed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88436948-fe05-4dad-ae27-06f4712efe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 2: Loại bỏ các đường link (token có chứa http, www)\n",
    "def remove_links(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if 'http' not in token and 'www' not in token]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(remove_links)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "\n",
    "print(f\"Total number of rows affected: {affected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4561df5-c765-4e88-846a-47d9f0a288d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 3: Loại bỏ các tokens dài hơn 16 ký tự\n",
    "def remove_long_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if len(token) <= 16]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(remove_long_tokens)\n",
    "\n",
    "removed_tokens_count = sum(len(original.split()) - len(cleaned.split()) for original, cleaned in zip(original_text, corpus['text']))\n",
    "\n",
    "print(f\"Total number of tokens removed: {removed_tokens_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c4e0459-2288-46b0-bd1e-8363feb63f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens where the last 'i' was removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 4: Xóa ký tự 'i' với mọi token kết thúc bằng ký tự 'i'\n",
    "def strip_last_i_count_changes(text):\n",
    "    tokens = text.split()\n",
    "    changes_made = 0\n",
    "    stripped_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.endswith('i'):\n",
    "            stripped_tokens.append(token[:-1])\n",
    "            changes_made += 1\n",
    "        else:\n",
    "            stripped_tokens.append(token)\n",
    "    \n",
    "    return ' '.join(stripped_tokens), changes_made\n",
    "\n",
    "corpus['text'], changes_made = zip(*corpus['text'].apply(strip_last_i_count_changes))\n",
    "\n",
    "total_changes_made = sum(changes_made)\n",
    "\n",
    "print(f\"Total number of tokens where the last 'i' was removed: {total_changes_made}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd4644b2-ec71-4210-bf20-46049b0f19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 5: Loại bỏ các tokens cụ thể dựa trên cơ sở phần phân tích n-Grams\n",
    "specific_words = ['filler', 'br', 'eve', 'pee', 'poo', 'ampxb', 'nowdrink', 'monitor', 'click', 'horny']\n",
    "\n",
    "def remove_specific_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if token not in specific_words and not token.startswith('gt')]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(remove_specific_tokens)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "\n",
    "print(f\"Total number of rows affected: {affected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45db4ee0-8293-4dc6-8e94-8c921312def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected by lemmatization: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 6: Thực hiện Lemmatization\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'] = corpus['text'].apply(lemmatize_text)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "print(f\"Total number of rows affected by lemmatization: {affected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "826c4a72-d16c-4152-b1c4-114be39e870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows affected by stop words removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 7: Loại bỏ stop-words\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'] = corpus['text'].apply(remove_stop_words)\n",
    "\n",
    "affected_rows = (original_text != corpus['text']).sum()\n",
    "print(f\"Total number of rows affected by stop words removal: {affected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25a40a84-8f65-4300-b359-5e6395c0d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total changes made in repeated character handling: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 8: Đưa những yếu tố có char lặp lại về 1 char (eg. soooooo -> so, dddddd -> d)\n",
    "def handle_repeated_chars(text):\n",
    "    cleaned_text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    changes_made = sum(1 for original, cleaned in zip(text, cleaned_text) if original != cleaned)\n",
    "    return cleaned_text, changes_made\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'], changes_made = zip(*corpus['text'].apply(handle_repeated_chars))\n",
    "\n",
    "total_changes = sum(changes_made)\n",
    "\n",
    "print(f\"Total changes made in repeated character handling: {total_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4a55c24-1414-4c25-aea4-ed198a71daae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total changes made in removing one-character tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 9: loại bỏ các tokens chỉ có 1 char\n",
    "def remove_one_char_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if len(token) > 1]\n",
    "    changes_made = len(tokens) - len(cleaned_tokens)\n",
    "    return ' '.join(cleaned_tokens), changes_made\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'], one_char_changes = zip(*corpus['text'].apply(remove_one_char_tokens))\n",
    "\n",
    "total_one_char_changes = sum(one_char_changes)\n",
    "print(f\"Total changes made in removing one-character tokens: {total_one_char_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e63e841d-1769-4bd6-aa02-5dd30dd4dccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total changes made in removing tokens with frequency less than 5: 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 10: Loại bỏ các tokens có tần suất nhỏ hơn 5\n",
    "all_tokens = ' '.join(corpus['text']).split()\n",
    "token_freq = pd.Series(all_tokens).value_counts()\n",
    "single_freq_tokens = set(token_freq[token_freq <= 4].index)\n",
    "\n",
    "def remove_single_freq_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [token for token in tokens if token not in single_freq_tokens]\n",
    "    changes_made = len(tokens) - len(cleaned_tokens)\n",
    "    return ' '.join(cleaned_tokens), changes_made\n",
    "\n",
    "original_text = corpus['text'].copy()\n",
    "corpus['text'], single_freq_changes = zip(*corpus['text'].apply(remove_single_freq_tokens))\n",
    "\n",
    "total_single_freq_changes = sum(single_freq_changes)\n",
    "print(f\"Total changes made in removing tokens with frequency less than 5: {total_single_freq_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e0323de-ddcf-4f7d-ac58-390ada63bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows removed (longer than 1249 tokens): 0\n"
     ]
    }
   ],
   "source": [
    "# Bước 11: Loại bỏ các sample được coi là dị thường dựa vào total tokens.\n",
    "max_tokens = 1249\n",
    "\n",
    "initial_row_count = len(corpus)\n",
    "corpus = corpus[corpus['text'].apply(lambda x: len(x.split()) <= max_tokens)]\n",
    "removed_rows = initial_row_count - len(corpus)\n",
    "\n",
    "print(f\"Total number of rows removed (longer than {max_tokens} tokens): {removed_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d286438b-6e4c-4732-b122-7c60e95a8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cleaned data: (27940, 2)\n",
      "\n",
      "Info of the cleaned data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 27940 entries, 0 to 27976\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    27940 non-null  object\n",
      " 1   label   27940 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 654.8+ KB\n",
      "None\n",
      "\n",
      "First 10 rows of the cleaned data:\n",
      "                                                text  label\n",
      "0  dear american teen question dutch person heard...      0\n",
      "1  nothing look forward life dont many reason kee...      1\n",
      "2  music recommendation im looking expand playlis...      0\n",
      "3  im done trying feel reason im still alive know...      1\n",
      "4  worried year old girl subject domestic going l...      1\n",
      "5  hey rredflag sure right place post go im curre...      1\n",
      "6  feel like someone need hear tonight feeling ri...      0\n",
      "7  deserve died right noone would care real frien...      1\n",
      "8  feel good ive set killing friday nice finally ...      1\n",
      "9  live made stupid random choice getting basical...      1\n"
     ]
    }
   ],
   "source": [
    "# Bước 12: Loại bỏ các sample trống hoặc chỉ chứa whitespace\n",
    "corpus = corpus[corpus['text'].str.strip() != '']\n",
    "\n",
    "# Thông tin của bộ corpus đã dọn\n",
    "print(\"Shape of the cleaned data:\", corpus.shape)\n",
    "print(\"\\nInfo of the cleaned data:\")\n",
    "print(corpus.info())\n",
    "print(\"\\nFirst 10 rows of the cleaned data:\")\n",
    "print(corpus.head(10))\n",
    "\n",
    "# Lưu lại bộ corpus đã dọn\n",
    "corpus.to_csv('data/cleaned_mhc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
